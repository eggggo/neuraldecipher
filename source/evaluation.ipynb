{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation script\n",
    "This notebook contains a minimal example for the reconstruction of ECFP$_6$ representations of length 1024 on the cluster split.  \n",
    "Make sure to execute the `source/run_cddd_inference_server.py` in another process within the `neuraldecipher` environment, such that the CDDD inference server can be used to decode the predicted cddd-representations back to SMILES representations.  \n",
    "To run the CDDD-server on three GPUs 0,1,2 with 6 parallel processes, execute:\n",
    "````\n",
    "python source/run_cddd_inference_server.py --device 0,1,2 --nservers 6\n",
    "\n",
    "````\n",
    "On the console following message should be printed out:\n",
    "```\n",
    "Using GPU devices: 0,1,2\n",
    "Total number of servers to spin up: 6\n",
    "Server running on GPU  0\n",
    "Server running on GPU  0\n",
    "Server running on GPU  1\n",
    "Server running on GPU  1\n",
    "Server running on GPU  2\n",
    "Server running on GPU  2\n",
    "```\n",
    "You can additionally check if the GPU-0 device is blocked by simply executing:\n",
    "`nvidia-smi`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load needed cddd modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cddd.inference import InferenceServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the CDDD-Inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = InferenceServer(port_frontend=5527, use_running=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load rest modules for reverse-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import rdBase\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load utility modules for dataloading and the Neuraldecipher class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_train_and_test_set, create_data_loaders, get_eval_data\n",
    "from models import Neuraldecipher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(ecfp_path, random_split=False):\n",
    "    train_data, test_data = create_train_and_test_set(ecfp_path, random_split=random_split)\n",
    "    train_loader, test_loader = create_data_loaders(train_data, test_data, batch_size=256, num_workers=5, shuffle_train=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(param_config_path, model_weights_path):\n",
    "    \"\"\"\n",
    "    Loads the neuraldecipher model\n",
    "    :param param_config_path [str] path to where the parameter configurations are stored\n",
    "    :param model_weights_path [str] path to where the model weights are stored\n",
    "    \"\"\"\n",
    "    with open(param_config_path, 'r', encoding='utf-8') as config_file:\n",
    "        json_string = config_file.read()\n",
    "    \n",
    "    print(\"Parameter configs:\")\n",
    "    print(json_string)\n",
    "    print(\"-\"*100)\n",
    "    print(\"Model:\")\n",
    "    params = json.loads(json_string)\n",
    "    nd_model = Neuraldecipher(**params['neuraldecipher'])\n",
    "    nd_model.load_state_dict(torch.load(model_weights_path))\n",
    "    print(nd_model)\n",
    "    return nd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter configs:\n",
      "{\n",
      "\t\"neuraldecipher\":{\n",
      "\t\"input_dim\": 1024,\n",
      "\t\"output_dim\": 512,\n",
      "\t\"layers\": [1024,768,512],\n",
      "\t\"dropout\": 0.0,\n",
      "\t\"normalization\": \"batch\",\n",
      "\t\"use_tanh\": true,\n",
      "\t\"activation\": \"relu\",\n",
      "\t\"norm_before\": false\n",
      "\t}\n",
      ",\n",
      "\n",
      "\t\"training\":{\n",
      "\t\t\"lr\": 0.00001,\n",
      "\t\t\"b1\": 0.9,\n",
      "\t\t\"b2\": 0.999,\n",
      "\t\t\"weight_decay\": 0.00005,\n",
      "\t\t\"loss\": \"log-cosh\",\n",
      "\t\t\"start_epoch\": 0,\n",
      "\t\t\"n_epochs\": 300,\n",
      "\t\t\"patience\": 10,\n",
      "\t\t\"batch_size\": 256,\n",
      "\t\t\"seed\": 42,\n",
      "\t\t\"device\": \"cuda:0\",\n",
      "\t\t\"output_dir\": \"1024_final_model_cs_gpu\",\n",
      "\t\t\"data_dir\": \"data/dfFold1024/ecfp6_train_c.npy\",\n",
      "\t\t\"radii\": 3\n",
      "\t}\n",
      "}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:\n",
      "Neuraldecipher(\n",
      "  (model): Sequential(\n",
      "    (linear_0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (activation_0): ReLU(inplace=True)\n",
      "    (batch_normalization_0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear_1): Linear(in_features=1024, out_features=768, bias=True)\n",
      "    (activation_1): ReLU(inplace=True)\n",
      "    (batch_normalization_1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear_2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (activation_2): ReLU(inplace=True)\n",
      "    (batch_normalization_2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear_3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (output_activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "neuraldecipher = load_model(\"../params/1024_config_count_gpu.json\",\n",
    "                           \"../models/1024_final_model_cs_gpu/weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set device for current `neuraldecipher`.\n",
    "If the Neuraldecipher fits into GPU memory with GPU:0 next to the CDDD inference server, you can allocate the model there.  \n",
    "However, we recommend using another GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\")\n",
    "neuraldecipher = neuraldecipher.eval()    \n",
    "neuraldecipher = neuraldecipher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_dataloader(dataloader, device, neuraldecipher, true_smiles=None):\n",
    "    \"\"\"\n",
    "    Computes a full forward pass on an entire dataset\n",
    "    :param dataloader [torch.utils.data.Dataloader] Torch dataloader that contains the batches\n",
    "    :param device [torch.device] Torch device where the computation should be performed on\n",
    "    :param neuraldecipher [Neuraldecipher] neuraldecipher model\n",
    "    :param true_smiles [None or list] List of true smiles representation. This variable is used when the dataloader\n",
    "                        does not contain the true SMILES representations within each batch.\n",
    "                        (The case when dealing with temporal split)\n",
    "    \"\"\"\n",
    "    predicted_cddd = []\n",
    "    with torch.no_grad():\n",
    "        if true_smiles is None:\n",
    "            true_smiles = []\n",
    "            for sample_batched in dataloader:\n",
    "                ecpf_in = sample_batched['ecfp'].to(device=device, dtype=torch.float32)\n",
    "                true_smiles.append(sample_batched['smiles'])\n",
    "                output = neuraldecipher(ecpf_in) \n",
    "                predicted_cddd.append(output.detach().cpu().numpy())\n",
    "            \n",
    "            true_smiles = np.concatenate(true_smiles)\n",
    "        else:\n",
    "            for batch in dataloader:\n",
    "                ecpf_in = batch.to(device=device, dtype=torch.float32)\n",
    "                output = neuraldecipher(ecpf_in) \n",
    "                predicted_cddd.append(output.detach().cpu().numpy())\n",
    "\n",
    "    predicted_cddd = np.concatenate(predicted_cddd)\n",
    "\n",
    "    return predicted_cddd, true_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility function to create batches from a large list\n",
    "def get_batches_from_large_list(large_list, batch_size):\n",
    "    n_batches = len(large_list) // batch_size\n",
    "    rest_indices = len(large_list) - n_batches*batch_size\n",
    "    last_start = n_batches*batch_size\n",
    "    last_end = last_start + rest_indices\n",
    "    batches = [large_list[i*batch_size:(i+1)*batch_size] for i in range(n_batches)]\n",
    "    batches.append(large_list[last_start:last_end])\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_sanitize_smiles(smiles, sanitize=True):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if sanitize:\n",
    "            Chem.SanitizeMol(mol)\n",
    "        smi = Chem.MolToSmiles(mol)\n",
    "        return smi\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(smi_true,\n",
    "                   smi_recon,\n",
    "                   radius=3, nbits=1024):\n",
    "    \"\"\"\n",
    "    For evaluation always compute the ECFP fingerprints on consistens lengths and radius.\n",
    "    Evaluation settings are ECFP6_1024\n",
    "    \"\"\"\n",
    "    mol_true = Chem.MolFromSmiles(smi_true)\n",
    "    mol_reconstructed = Chem.MolFromSmiles(smi_recon)\n",
    "    #fingerprint similarity according to ECFP_fixed\n",
    "    fp1_ecfp = AllChem.GetHashedMorganFingerprint(mol_true, radius=radius, nBits=nbits)\n",
    "    fp2_ecfp = AllChem.GetHashedMorganFingerprint(mol_reconstructed, radius=radius, nBits=nbits)\n",
    "    tanimoto_ecfp = DataStructs.TanimotoSimilarity(fp1_ecfp, fp2_ecfp)\n",
    "\n",
    "    return tanimoto_ecfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Since we are using 6 CDDD inference servers, we can set the pool of workers to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_cddd(batch_list, npool=6):\n",
    "    with Pool(npool) as pool:\n",
    "        decoded_smiles = pool.map(inference_server.emb_to_seq, batch_list)\n",
    "    return decoded_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecfp_path_validationset = \"data/dfFold1024/ecfp6_train_c.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, validation_loader = get_data_loaders(ecfp_path_validationset, random_split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the temporal data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_dataloader, temporal_smiles = get_eval_data(ecfp_path='data/dfFold1024/ecfp6_temporal_c.npy',\n",
    "                                                     smiles_path='data/smiles_temporal.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_wrapper(neuraldecipher, dataloader, true_smiles):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # compute full forwardpass of dataloader\n",
    "    print(\"Predicting cddd representations...\")\n",
    "    predicted_cddd, true_smiles = forward_pass_dataloader(dataloader, device, neuraldecipher, true_smiles)\n",
    "    # retrieve string representations with CDDD-decoder network\n",
    "    predicted_cddd = get_batches_from_large_list(predicted_cddd, 1024)\n",
    "    print(\"Decoding predicted cddd representations...\")\n",
    "    decoded_smiles = decode_cddd(predicted_cddd)\n",
    "    decoded_smiles = np.concatenate(decoded_smiles)\n",
    "    # canonicalize if possible, returns canonical smiles or None.\n",
    "    canonical_smiles = [canonicalize_sanitize_smiles(s, sanitize=True) for s in decoded_smiles]\n",
    "    # check valid SMILES\n",
    "    valid_ids = [i for i, smi in enumerate(canonical_smiles) if smi!= None]\n",
    "    validity = len(valid_ids)/len(canonical_smiles)\n",
    "    print(f\"Dataset size: {len(decoded_smiles)}.\")\n",
    "    print(f\"Validity of the reconstruction: {np.round(validity, 4)}.\")\n",
    "    valid_recon_smiles = decoded_smiles[valid_ids]\n",
    "    valid_true_smiles = true_smiles[valid_ids]\n",
    "    # check reconstruction accuracy\n",
    "    reconstruction_acc = np.sum([a==b for a,b in zip(valid_recon_smiles, valid_true_smiles)])/len(valid_ids)\n",
    "    print(f\"Reconstruction accuracy: {np.round(reconstruction_acc, 4)}.\")\n",
    "    # get Tanimoto similarity \n",
    "    tanimoto_sim = [get_similarity(smi_true, smi_recon) for smi_true, smi_recon in zip(valid_true_smiles,\n",
    "                                                                                   valid_recon_smiles)]\n",
    "    print(f\"Tanimoto similarity: {np.round(np.mean(tanimoto_sim), 4)}.\")\n",
    "    \n",
    "    \n",
    "    res_dict = dict()\n",
    "    res_dict[\"true_smiles\"] = valid_true_smiles\n",
    "    res_dict[\"recon_smiles\"] = valid_recon_smiles\n",
    "    res_dict[\"tanimoto_sim\"] = tanimoto_sim\n",
    "    \n",
    "    return {\"validity\": validity, \"recon_acc\": reconstruction_acc, \"res_dict\": res_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Validation dataset from the cluster split (112K samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting cddd representations...\n",
      "Decoding predicted cddd representations...\n",
      "Dataset size: 112332.\n",
      "Validity of the reconstruction: 0.9881.\n",
      "Reconstruction accuracy: 0.2227.\n",
      "Tanimoto similarity: 0.6139.\n",
      "CPU times: user 2min 51s, sys: 3.86 s, total: 2min 55s\n",
      "Wall time: 5min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_validation_nd1024_count = eval_wrapper(neuraldecipher, validation_loader, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Temporal dataset from ChEMBL26 (55K samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting cddd representations...\n",
      "Decoding predicted cddd representations...\n",
      "Dataset size: 55701.\n",
      "Validity of the reconstruction: 0.9712.\n",
      "Reconstruction accuracy: 0.1941.\n",
      "Tanimoto similarity: 0.5906.\n",
      "CPU times: user 1min 18s, sys: 2 s, total: 1min 20s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_temporal_nd1024_count = eval_wrapper(neuraldecipher, temporal_dataloader, temporal_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop the CDDD inference server execution in your other shell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "13ea6601ea5ff2400951d1e1b1fece15e1993651e3334696a8d76398a6bea111"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
